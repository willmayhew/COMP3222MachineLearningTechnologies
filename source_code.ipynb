{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "     # TO LOWER CASE\n",
    "    text = text.lower()\n",
    "\n",
    "    # REMOVE EMOJIS\n",
    "    def remove_emoji(string):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', string)\n",
    "\n",
    "    # REMOVE URLs\n",
    "    def remove_URLs(text):\n",
    "        first = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return re.sub(r\"\\\\\\/\\S+\", \"\", first)\n",
    "\n",
    "    # REMOVE USERNAME MENTIONS\n",
    "    def remove_Mentions(text):\n",
    "        return re.sub(r\"@\\w+\",\"\", text)\n",
    "\n",
    "    # REMOVE STOP WORDS\n",
    "    def remove_stopwords(text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        filtered_text = ' '.join(filtered_words)\n",
    "        return filtered_text\n",
    "\n",
    "\n",
    "    # REMOVE SPECIAL CHARACTERS\n",
    "    def clean_text(text):\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    def stem_words(text):\n",
    "        \n",
    "        porter = PorterStemmer()\n",
    "        words = word_tokenize(text)\n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "\n",
    "        return ' '.join(stemmed_words)\n",
    "    \n",
    "    def lemmatize_words(text):\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return ' '.join(lemmatized_words)\n",
    "    \n",
    "    text = remove_emoji(text)\n",
    "    text = remove_URLs(text)\n",
    "    text = remove_Mentions(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = clean_text(text)\n",
    "    text = stem_words(text)\n",
    "    text = lemmatize_words(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(y_true, y_pred):\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=['fake','real'])\n",
    "    TP, FP, FN, TN = conf_matrix.ravel()\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    print(\"TP: %d FP: %d TN: %d FN: %d\" % (TP, FP, TN, FN))\n",
    "    print(\"Precision: %0.3f\" % precision)\n",
    "    print(\"Recall: %0.3f\" % recall)\n",
    "    print(\"F1 Score: %0.3f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training and test data\n",
    "train_df = pd.read_csv(\"dataset/mediaeval-2015-trainingset.txt\", sep=\"\\t\")\n",
    "test_df = pd.read_csv(\"dataset/mediaeval-2015-testset.txt\", sep=\"\\t\")\n",
    "\n",
    "train_df = pd.DataFrame(data=train_df)\n",
    "test_df = pd.DataFrame(data=test_df)\n",
    "\n",
    "# print(train_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA ANALYSIS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              tweetId  \\\n",
      "0  263046056240115712   \n",
      "1  262995061304852481   \n",
      "2  262979898002534400   \n",
      "3  262996108400271360   \n",
      "4  263018881839411200   \n",
      "5  263364439582060545   \n",
      "6  262927032705490944   \n",
      "7  263321078884077568   \n",
      "8  263111677485142017   \n",
      "9  262977091983785985   \n",
      "\n",
      "                                                                                         tweetText  \\\n",
      "0  se acuerdan de la pelcula el da despus de maana recuerda lo que est pasando con el huracn sandy   \n",
      "1          miren sandy en ny tremenda imagen del huracn parece el da de la independencia 2 real rt   \n",
      "2              buena la foto del huracn sandy recuerda la pelcula da de la independencia id4 sandy   \n",
      "3                                                                          scary shit hurricane ny   \n",
      "4                                             fave place world nyc hurricane sandy statueofliberty   \n",
      "5                                                            42nd time square nyc subway hurricane   \n",
      "6                                                time halloween photo hurricane sandy frankenstorm   \n",
      "7                                     crazy pic hurricane sandy prayer go family friend east coast   \n",
      "8                                                      sandy newyork hurricane statueofliberty usa   \n",
      "9                                                                                    nyc hurricane   \n",
      "\n",
      "      userId      imageId(s)        username                       timestamp  \\\n",
      "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
      "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
      "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
      "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
      "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
      "5  163674788  sandyA_fake_23        classycg  Tue Oct 30 19:39:10 +0000 2012   \n",
      "6  246153081  sandyA_fake_14        j_unit87  Mon Oct 29 14:41:04 +0000 2012   \n",
      "7  199565482  sandyA_fake_29     MrBlakMagik  Tue Oct 30 16:46:52 +0000 2012   \n",
      "8   78475739  sandyA_fake_15          safi37  Tue Oct 30 02:54:46 +0000 2012   \n",
      "9  869777653  sandyA_fake_29   kingmichael03  Mon Oct 29 17:59:59 +0000 2012   \n",
      "\n",
      "  label  \n",
      "0  fake  \n",
      "1  fake  \n",
      "2  fake  \n",
      "3  fake  \n",
      "4  fake  \n",
      "5  fake  \n",
      "6  fake  \n",
      "7  fake  \n",
      "8  fake  \n",
      "9  fake  \n"
     ]
    }
   ],
   "source": [
    "#Pre-Processing\n",
    "\n",
    "# Replace 'humor' label with 'fake' -------------------------------------------------------------------\n",
    "train_df['label'] = train_df['label'].replace('humor', 'fake')\n",
    "test_df['label'] = test_df['label'].replace('humor', 'fake')\n",
    "\n",
    "# # REMOVE DUPLICATES | MAYBE DONT REMOVE DUPLICATES AS IT RESEMBLES REALISM -------------------------------------------------------------------\n",
    "# train_data = train_data.drop_duplicates(subset=['tweetText'])\n",
    "\n",
    "train_df['tweetText'] = train_df['tweetText'].apply(preprocess_text)\n",
    "test_df['tweetText'] = test_df['tweetText'].apply(preprocess_text)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(train_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "tfidfVectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidfVectorizer.fit_transform(train_df['tweetText'])\n",
    "X_test_tfidf = tfidfVectorizer.transform(test_df['tweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2044  502]\n",
      " [ 330  879]]\n",
      "TP: 2044 FP: 502 TN: 879 FN: 330\n",
      "Precision: 0.803\n",
      "Recall: 0.861\n",
      "F1 Score: 0.831\n"
     ]
    }
   ],
   "source": [
    "# # Create a Random Forest classifier\n",
    "# rfc_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train the Random Forest classifier\n",
    "# rfc_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# rfcPrediction_tfid = rfc_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# getMetrics(y_test, rfcPrediction_tfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply PCA for dimensionality reduction\n",
    "# n_components = 1000  # Number of components to reduce to (adjust as needed)\n",
    "# pca = PCA(n_components=n_components)\n",
    "# X_train_reduced = pca.fit_transform(X_train_count.toarray())  # Fit PCA on training data\n",
    "# X_test_reduced = pca.transform(X_test_count.toarray())  # Transform test data using the same PCA\n",
    "\n",
    "# # Display the reduced dimensions\n",
    "# print(\"Original shape:\", X_train_count.shape)\n",
    "# print(\"Reduced shape:\", X_train_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramVectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X_train_ngram = ngramVectorizer.fit_transform(train_df['tweetText'])\n",
    "X_test_ngram = ngramVectorizer.transform(test_df['tweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2312  234]\n",
      " [ 374  835]]\n",
      "TP: 2312 FP: 234 TN: 835 FN: 374\n",
      "Precision: 0.908\n",
      "Recall: 0.861\n",
      "F1 Score: 0.884\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest classifier\n",
    "rfc_ngram = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rfc_ngram.fit(X_train_ngram, y_train)\n",
    " \n",
    "# Make predictions on the test set\n",
    "rfcPrediction_ngram = rfc_ngram.predict(X_test_ngram)\n",
    "\n",
    "getMetrics(y_test, rfcPrediction_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2111  435]\n",
      " [ 324  885]]\n",
      "TP: 2111 FP: 435 TN: 885 FN: 324\n",
      "Precision: 0.829\n",
      "Recall: 0.867\n",
      "F1 Score: 0.848\n"
     ]
    }
   ],
   "source": [
    "# Create a Naive Bayes classifier\n",
    "nbc_tfidf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nbc_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "nbcPrediction_tfidf = nbc_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "getMetrics(y_test, nbcPrediction_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
